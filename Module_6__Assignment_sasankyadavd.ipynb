{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> Title : Module 6 Assignment\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Name : Sasank Yadav Daliboyina\n",
        "\n",
        "Date : 12.17.2023\n",
        "\n",
        "Professor : Mitch Harris\n",
        "\n",
        "EAI 6000-Fundamentals of Artificial Intelligence.\n",
        "\n",
        "Northeastern University.\n",
        "\n",
        "NUId : 002612278\n"
      ],
      "metadata": {
        "id": "ikPD6F_tzZzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCTION :**  In this exploration, we delved into Natural Language Processing (NLP) using Python's NLTK library, applying it to classic literature from Project Gutenberg, specifically Jane Austen's \"Emma\". The process began with essential NLP tasks like tokenization, to break the text into manageable segments, and continued with the removal of stopwords, enhancing the focus on meaningful content. Stemming was applied for word simplification, and lemmatization further refined this by contextually analyzing words. The Bag of Words model vectorized the text, preparing it for machine learning applications. This foundational work in text processing and analysis exemplifies key techniques in NLP, crucial for understanding and manipulating language data."
      ],
      "metadata": {
        "id": "JSwtRtmt5B83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import and Download Necessary NLTK Resources**"
      ],
      "metadata": {
        "id": "OMKC_z2yzmVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igh3zRwbyBjy",
        "outputId": "bc830bb3-77b7-4546-dc48-fe7f33a1b073"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Explore the Gutenberg Corpus**"
      ],
      "metadata": {
        "id": "uF5svd81z0q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available texts in the Gutenberg Corpus\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "# Choose \"Emma\" by Jane Austen\n",
        "austen_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Display the first 500 characters\n",
        "print(austen_text[:500])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks4p2R42yFf9",
        "outputId": "c0205a78-602b-4945-dcd5-f8a2d35b0228"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Tokenization**"
      ],
      "metadata": {
        "id": "JkjFlhxTz4Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(austen_text[:5000])  # Limiting to the first 5000 characters\n",
        "print(\"First 5 sentences:\", sentences[:5])\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(austen_text[:5000])\n",
        "print(\"First 50 words:\", words[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jTXRMAgyJMr",
        "outputId": "bf341742-5b5b-4bea-e96c-03e89af5836e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences: ['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\", 'Her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.', \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of Emma.\", 'Between _them_ it was more the intimacy\\nof sisters.']\n",
            "First 50 words: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Removing Stopwords**"
      ],
      "metadata": {
        "id": "36TLDGHE0ADF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stopwords from the first 500 words\n",
        "filtered_words = [word for word in words[:500] if word.lower() not in stop_words]\n",
        "print(\"First 50 words without stopwords:\", filtered_words[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SAqja8DyOWZ",
        "outputId": "ae994f4c-2b4a-4512-f2f8-687666a2ce01"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 words without stopwords: ['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy', 'disposition', ',', 'seemed', 'unite', 'best', 'blessings', 'existence', ';', 'lived', 'nearly', 'twenty-one', 'years', 'world', 'little', 'distress', 'vex', '.', 'youngest', 'two', 'daughters', 'affectionate', ',', 'indulgent', 'father', ';', ',', 'consequence', 'sister', \"'s\", 'marriage']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Stemming**"
      ],
      "metadata": {
        "id": "9LrRhOHH0HOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
        "print(\"First 50 stemmed words:\", stemmed_words[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSU5iCKzyRjJ",
        "outputId": "10fea497-fa4c-4021-924e-8d22f7717fc6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 stemmed words: ['[', 'emma', 'jane', 'austen', '1816', ']', 'volum', 'chapter', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'rich', ',', 'comfort', 'home', 'happi', 'disposit', ',', 'seem', 'unit', 'best', 'bless', 'exist', ';', 'live', 'nearli', 'twenty-on', 'year', 'world', 'littl', 'distress', 'vex', '.', 'youngest', 'two', 'daughter', 'affection', ',', 'indulg', 'father', ';', ',', 'consequ', 'sister', \"'s\", 'marriag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: From Strings to Vectors (Bag of Words)**"
      ],
      "metadata": {
        "id": "83YoUQ6C0mHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Creating a bag of words\n",
        "bow = Counter(stemmed_words)\n",
        "print(\"Bag of Words Count:\", bow.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ulOSrVyVEJ",
        "outputId": "313b30f4-dcc8-4da3-e42b-cb04ac2bcc5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Count: [(',', 31), ('.', 15), (';', 8), ('emma', 6), (\"'s\", 6), ('miss', 5), ('taylor', 5), ('friend', 5), ('littl', 3), ('father', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Vectorization with sklearn**"
      ],
      "metadata": {
        "id": "lfPD_51r0sUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing using CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vectorized text shape:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsoiwUCrzCR1",
        "outputId": "4c842bd9-ee30-40b8-be2d-16d4fdc517ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized text shape: (26, 389)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Lemmatization**\n",
        "\n",
        "Lemmatization is a more sophisticated approach than stemming. It considers the context and converts the word to its meaningful base form."
      ],
      "metadata": {
        "id": "AnUJcQny03gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert the part-of-speech naming scheme from the Penn Treebank tag to a WordNet tag.\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tag(words[:500])]\n",
        "print(\"Lemmatized words:\", lemmatized[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhWbqTxSzGDc",
        "outputId": "5f4f027d-ca61-4735-ff91-05df5a73f0fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seem', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', ';', 'and', 'have', 'live', 'nearly', 'twenty-one', 'year', 'in', 'the', 'world', 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Using a Stronger List of Stopwords**\n",
        "\n",
        "Combining NLTK's stopwords with other sources for a more comprehensive list:"
      ],
      "metadata": {
        "id": "mcUROqVN3XQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using an external list of stopwords\n",
        "extra_stopwords = ['example', 'stopword1', 'stopword2']  # This is an illustrative list\n",
        "\n",
        "all_stopwords = set(stopwords.words('english')).union(set(extra_stopwords))\n",
        "filtered_words = [word for word in lemmatized if word.lower() not in all_stopwords]\n",
        "print(\"Filtered words:\", filtered_words[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnpUlpGa3WqV",
        "outputId": "35a45eb4-1d0d-4d55-b2f7-69b8968fd629"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words: ['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy', 'disposition', ',', 'seem', 'unite', 'best', 'blessing', 'existence', ';', 'live', 'nearly', 'twenty-one', 'year', 'world', 'little', 'distress', 'vex', '.', 'young', 'two', 'daughter', 'affectionate', ',', 'indulgent', 'father', ';', ',', 'consequence', 'sister', \"'s\", 'marriage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "In the above analysis, we employed several fundamental natural language processing (NLP) techniques to process and analyze text data, using Python's NLTK library and texts from the Project Gutenberg corpus. Specifically, we:\n",
        "\n",
        "Downloaded and Explored Text Data: We initially aimed to use \"Pride and Prejudice\" by Jane Austen but switched to \"Emma\" by the same author due to dataset availability. This change demonstrates the flexibility required in data analysis when dealing with text corpora.\n",
        "\n",
        "Tokenization: We split the text into sentences and words. This step is crucial for breaking down large text blocks into manageable units for further analysis.\n",
        "\n",
        "Stopwords Removal: We filtered out common stopwords (words with little semantic value) from our text data. This process helps focus on the more meaningful content in the text.\n",
        "\n",
        "Stemming: We applied stemming to reduce words to their base or root form. This technique simplifies the dataset and aids in consolidating different forms of a word.\n",
        "\n",
        "Vectorization (Bag of Words): We transformed the processed text into a numerical format using a bag-of-words model, making it suitable for machine learning algorithms.\n",
        "\n",
        "Lemmatization: As an advanced step, we performed lemmatization to accurately reduce words to their dictionary form, considering the context. This process is more sophisticated than stemming and provides a deeper level of text normalization.\n",
        "\n",
        "Advanced Stopword Handling: We explored the idea of extending the list of stopwords by combining NLTK's list with additional sources to create a more comprehensive filter.\n",
        "\n",
        "Application to a Machine Learning Task: We implemented a Naive Bayes classifier using the Iris dataset as an example to demonstrate how preprocessed text data can be used in machine learning tasks.\n",
        "\n",
        "Throughout this process, we demonstrated key NLP techniques and their applications in text analysis and machine learning. These methods form the foundation of many advanced NLP applications and are essential for anyone looking to delve into text analytics or develop NLP-driven models."
      ],
      "metadata": {
        "id": "0gkVeEw431uh"
      }
    }
  ]
}